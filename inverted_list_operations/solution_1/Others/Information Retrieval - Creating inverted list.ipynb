{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import en_core_web_sm\n",
    "from pattern.en import suggest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"A Boy's Will - Robert Frost/1.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_new = init_process(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(txt):\n",
    "    no_new = re.sub('\\n', ' ', txt)\n",
    "    no_spl = re.sub('Ã±', ' ', no_new)\n",
    "    first_parse = re.sub(r'[^\\w]', ' ', no_spl)\n",
    "    #word_tokens = word_tokenize(first_parse)\n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return first_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(w):\n",
    "    word_wlf = reduce_lengthening(w)\n",
    "    correct_word = suggest(word_wlf)\n",
    "    return correct_word[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(words):\n",
    "    correct = [correct_spelling(w) for w in words]\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    #filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    read_txt = file.read()\n",
    "    list_new = init_process(read_txt)\n",
    "    stemmed = stemming(list_new)\n",
    "    lemma = lemmatize_text(stemmed)\n",
    "    new_words = remove_stopwords(lemma)\n",
    "    final = spelling_correction(new_words)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_text():\n",
    "    for i in range(1,51):\n",
    "        new_text_file = str(i)+\".txt\"\n",
    "        file_name = \"poems/\"+new_text_file\n",
    "        refined_list = process(file_name)\n",
    "        refined_text = ' '.join(refined_list)\n",
    "        text_file = open((\"refined/\"+new_text_file), \"w\")\n",
    "        text_file.write(refined_text)\n",
    "        text_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine_text() # Dont refine everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for i in range(1,51):\n",
    "    file_name = (\"refined/\"+str(i)+\".txt\")\n",
    "    text = open(file_name, \"r\").read()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for t in tokens:\n",
    "        if t in dictionary:\n",
    "            dictionary.get(t).append(i)\n",
    "            dictionary[t] = list(set(dictionary.get(t)))\n",
    "        else:\n",
    "            dictionary[t] = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont do everytime\n",
    "# text_file = open(\"inverted_list.csv\", \"w\")\n",
    "# text_file.write(\"Words, Inverted Index \\n\")\n",
    "# for item in dictionary.keys():\n",
    "#     join_items = \", \".join(str(d) for d in dictionary.get(item))\n",
    "#     text = item+\", \"+str(join_items)\n",
    "#     text_file.write(text+\" \\n\")\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to make inverted index - not working\n",
    "# import InvertedIndex\n",
    "# import InvertedIndexQuery\n",
    "\n",
    "# i = InvertedIndex.Index()\n",
    "\n",
    "# filename = '1.txt'\n",
    "# file_to_index = open(filename).read() \n",
    "# document_key = filename\n",
    "\n",
    "#     # index the document, using document_key as the document's\n",
    "#     # id.\n",
    "# i.index(file_to_index, document_key)\n",
    "\n",
    "# filename = '2.txt'\n",
    "# file_to_index = open(filename).read()\n",
    "# document_key = filename\n",
    "\n",
    "# i.index(file_to_index, document_key)\n",
    "\n",
    "# search_results = InvertedIndexQuery.query('Python and spam', i)\n",
    "# search_results.sort()\n",
    "\n",
    "# cnt = 0\n",
    "# for document in search_results:\n",
    "#     cnt = cnt + 1\n",
    "#     print ('%d) %s'.format(cnt, document[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
