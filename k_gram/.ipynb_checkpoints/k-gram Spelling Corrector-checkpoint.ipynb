{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import en_core_web_sm\n",
    "from pattern.en import suggest\n",
    "import pandas as pd\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(txt):\n",
    "    no_new = re.sub('\\n', ' ', txt)\n",
    "    no_spl = re.sub('Ã±', ' ', no_new)\n",
    "    first_parse = re.sub(r'[^\\w]', ' ', no_spl)\n",
    "    #word_tokens = word_tokenize(first_parse)\n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return remove_nums(first_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    #filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    read_txt = file.read()\n",
    "    list_new = init_process(read_txt)\n",
    "    #stemmed = stemming(list_new)\n",
    "    #lemma = lemmatize_text(stemmed)\n",
    "    new_words = remove_stopwords(list_new)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nums(text):\n",
    "    no_new = re.sub('[0-9]', '', text)\n",
    "    return no_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_text():\n",
    "    file_name = \"india.txt\"\n",
    "    refined_list = process(file_name)\n",
    "    refined_text = ' '.join(refined_list)\n",
    "    text_file = open(\"india_ref_1.txt\", \"w\")\n",
    "    text_file.write(refined_text)\n",
    "    text_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prodBigrams(text):\n",
    "    spaced = ''\n",
    "    n = 2\n",
    "    for ch in text:\n",
    "        spaced = spaced + ch + ' '\n",
    "    tokenized = spaced.split(\" \")\n",
    "    myList = list(ngrams(tokenized, n))\n",
    "    Bigrams = []\n",
    "    for i in myList:\n",
    "        Bigrams.append((''.join([w + '' for w in i])).strip())\n",
    "    return Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "file_name = (\"india_ref_1.txt\")\n",
    "text = open(file_name, \"r\").read()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "for i in range(0,len(tokens)):\n",
    "    t = tokens[i]\n",
    "    new_tok = prodBigrams(t)\n",
    "    for nt in new_tok:\n",
    "        if nt in dictionary:\n",
    "            dictionary.get(nt).append(i+1)\n",
    "            dictionary[nt] = list(set(dictionary.get(nt)))\n",
    "        else:\n",
    "            dictionary[nt] = [i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dictionary.items():\n",
    "    dictionary[key] = sorted(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont do everytime\n",
    "# text_file = open(\"list.csv\", \"w\")\n",
    "# text_file.write(\"Bigrams, Inverted Index \\n\")\n",
    "# for item in dictionary.keys():\n",
    "#     join_items = \", \".join(str(d) for d in dictionary.get(item))\n",
    "#     text = item+\", \"+str(join_items)\n",
    "#     text_file.write(text+\" \\n\")\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_intersect(list1, list2):\n",
    "    mer_list = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while (i<len(list1) and j<len(list2)):\n",
    "        if (list1[i] == list2[j]):\n",
    "            mer_list.append(list1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if (list1[i] > list2[j]):\n",
    "                j = j+1\n",
    "            else:\n",
    "                i = i+1\n",
    "    return mer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mer_list = []\n",
    "# for i in range(0,(len(kgrams))):\n",
    "#     for j in range(i,(len(kgrams))):\n",
    "#         mer_list.append(and_intersect(dictionary.get(kgrams[i]), dictionary.get(kgrams[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_term(word):\n",
    "    word = 'Indya'\n",
    "    kgrams = prodBigrams(word)\n",
    "    count_list = {}\n",
    "    for i in range(0,len(kgrams)):\n",
    "        if (kgrams[i] in dictionary):\n",
    "            for t in dictionary.get(kgrams[i]):\n",
    "                if t in count_list:\n",
    "                    count_list[t] = (count_list.get(t)+1)\n",
    "                else:\n",
    "                    count_list[t] = 1\n",
    "    return get_suggestions(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(count_list):\n",
    "    sorted_list = sorted(count_list.items(), key=lambda x: x[1], reverse=True)\n",
    "    max_val = list(sorted_list)[0][1]\n",
    "    file_name = (\"india_ref_1.txt\")\n",
    "    text = open(file_name, \"r\").read()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    suggestions = []\n",
    "    temp = max_val\n",
    "    for i in iter(sorted_list):\n",
    "        temp = i[1]\n",
    "        if (temp == max_val):\n",
    "            suggestions.append(tokens[(i[0]-1)])\n",
    "        else:\n",
    "            break\n",
    "    return set(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'India', 'Indonesia', 'Pandyas'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_term('Indiya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
