{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import sys\n",
    "import array\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_filenames = {0 : \"documents/lotr.txt\",\n",
    "                      1 : \"documents/silmarillion.txt\",\n",
    "                      2 : \"documents/rainbows_end.txt\",\n",
    "                      3 : \"documents/the_hobbit.txt\",\n",
    "                      4 : \"documents/deadpool.txt\",\n",
    "                      5 : \"documents/deadpool_2.txt\",\n",
    "                      6 : \"documents/infinity_war.txt\",\n",
    "                      7 : \"documents/age_ultron.txt\",\n",
    "                      8 : \"documents/spotlight.txt\",\n",
    "                      9 : \"documents/bohemian_rhapsody.txt\",\n",
    "                      10 : \"documents/snowden.txt\"}\n",
    "\n",
    "\n",
    "N = len(document_filenames)\n",
    "\n",
    "dictionary = set()\n",
    "\n",
    "postings = defaultdict(dict)\n",
    "\n",
    "document_frequency = defaultdict(int)\n",
    "\n",
    "length = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    terms = text.lower().split()\n",
    "    char = \" .,!#$%^&*();:\\n\\t\\\\\\\"?!{}[]<>\"\n",
    "    return [t.strip(char) for t in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text():\n",
    "    global dictionary, postings\n",
    "    for id in document_filenames:\n",
    "        f = open(document_filenames[id],'r')\n",
    "        docs = f.read()\n",
    "        f.close()\n",
    "        terms = tokenize(docs)\n",
    "        unique_terms = set(terms)\n",
    "        dictionary = dictionary.union(unique_terms)\n",
    "        for t in unique_terms:\n",
    "            postings[t][id] = terms.count(t) # the value is the frequency of the term in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    # Counts the number of documents each term apperars in and stores it in document_frequency[term]\n",
    "    # It also computes the length of each element term\n",
    "    global document_frequency, length\n",
    "    for term in dictionary:\n",
    "        document_frequency[term] = len(postings[term])\n",
    "    for id in document_filenames:\n",
    "        l = 0\n",
    "        for term in dictionary:\n",
    "            l = l + (importance(term,id)**2)\n",
    "        length[id] = math.sqrt(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(term,id):\n",
    "    # Gives the importance og the term. If not in the doc 0 is given out.\n",
    "    if id in postings[term]:\n",
    "        return postings[term][id]*idf(term)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term):\n",
    "    # Returns the inverse document frequency of the term\n",
    "    if term in dictionary:\n",
    "        return math.log(N/document_frequency[term],2)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_search(q):\n",
    "    arr = array.array('f', [0,0])\n",
    "    query = tokenize(q)\n",
    "    relevant_document_ids = intersection([set(postings[term].keys()) for term in query])\n",
    "    if not relevant_document_ids:\n",
    "        print(\"No documents.\")\n",
    "    else:\n",
    "        scores = sorted([(id,cosine_similarity(query,id)) for id in relevant_document_ids], key=lambda x: x[1], reverse=True)\n",
    "        for (id,score) in scores:\n",
    "            doc = document_filenames[id].split('/')[1]\n",
    "            print(\"{} : {}\".format(doc, str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_table(query):\n",
    "#     text_file = open(str(query)+\".csv\", \"w\")\n",
    "#     text_file.write(\"Doc ID, Document Name, tf, df, idf, importance, cosine \\n\")\n",
    "#     for i in range(1, N+1):\n",
    "#         doc_det = str(i)+\", \"+document_filenames[i-1].split('/')[1]+\", \"\n",
    "#         freq = str(length[i])+\", \"+\n",
    "#     scores = sorted([(id,cosine_similarity(query,id)) for id in relevant_document_ids], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(sets):\n",
    "    # intersection of all sets in the list\n",
    "    inter = reduce(set.intersection, [s for s in sets])\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query,id):\n",
    "    # Compute the cosine similarity between query and document id\n",
    "    similarity = 0.0\n",
    "    for term in query:\n",
    "        if term in dictionary:\n",
    "            similarity = similarity + (idf(term)*importance(term,id))\n",
    "    similarity = similarity / length[id]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search():\n",
    "    read_text()\n",
    "    initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_ultron.txt : 0.27493211630377606\n"
     ]
    }
   ],
   "source": [
    "vector_search()\n",
    "do_search('where is powerful objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
